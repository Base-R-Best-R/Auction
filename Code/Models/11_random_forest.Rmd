---
title: |
       | 10 Random Forest
author: "Fabian Blasch"
date: "`r format(Sys.Date(), format = '%m/%d/%Y')`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{float}
   - \usepackage{titling}
   - \usepackage{xcolor}
output: 
   pdf_document:
      number_sections: TRUE
---

# Load Data

```{r, results = "hide", message = FALSE, warning = FALSE}
# source AUX
source("./../Misc/Auxilliary.R")
source("./../Misc/model_eval.R")

# packages
get.package(c("lubridate", "glmnet", "glmnetUtils", "tidyverse",
              "ranger", "caret", "beepr"))

# load data
dat_aucs <- readRDS("./../../Data/Bid Tab RDS/Aucs_df_split.RDS")
dat_aucs_eng <- readRDS("./../../Data/Bid Tab RDS/Aucs_df_feateng_split.RDS")
```

# Data

```{r}
# exclude variables that are not supposed to be in the model
lapply(dat_aucs_eng, \(x){

  # remove
  within(x, {
    Contract_ID <- NULL
    MLOT <- NULL
    EW_Diff <- NULL
    Winning_Bid <- Winning_Bid / 1e3
    Eng_Est <- Eng_Est / 1e3
  }) -> tmp
  
  # return
  return(tmp)
  
}) -> dat_aucs_mod
```

# Variable Selection

The data set contains a lot of binary variables that are highly correlated. Accordingly, to allow for a good prediction we need to perform variable selection within our cross validation procedure, commonly refered to as recursive feature elimination.

# Variable Selection 

```{r}
# write CV function for recursive feature elimination
rfe_rf_CV_par <- \(data, nfolds = 5, splitrule_CV = "variance",
                   min_node_size_CV = 1:4, max_depth_CV = seq(5, 70, 5),
                   num_trees_CV = 1500, var_share = 0.75, 
                   nrounds = seq(2, 6), importance = "permutation",
                   feat_share_CV = seq(0.3, 1, 0.2),
                   eval_fun = Metrics::rmse,
                   ncore = NULL,
                   seed = 33){

  # hot encode data
  data <- model.matrix(~. + 0, data = data) |> as.data.frame()
  
  # ensure viable colnames
  names(data) <- make.names(names(data))
  
  # reprod
  set.seed(seed)

  # generate CV folds
  folds <- sample(nrow(data), nrow(data), replace = FALSE) |> 
            split(as.factor(1:nfolds)) |> setNames(1:nfolds) |> suppressWarnings()
  
  # set up parallel compute cluster
  if(is.null(ncore)){
    
    # set amount of cores to the max available and leave one out
    ncore <- parallel::detectCores() - 1
    
    # we parallelize over folds - the maximum number of occupied cores should thus be
    ncore <- min(ncore, nfolds)
    
  } else {
    
    # find min of ncore and folds
    ncore <- min(ncore, nfolds)
    
  }
  
  # set up cluster
  clust <- parallel::makeCluster(ncore, outfile = "")

  # tuning grid for forest
  tgrid_RF <- expand.grid("feat_share" = feat_share_CV,
                          "splitrule" = splitrule_CV,
                          "min_node_size" = min_node_size_CV,
                          "max_depth" = max_depth_CV, 
                          "num_trees" = num_trees_CV,
                          "nrounds" = nrounds)
  
  # print cores that will be occupied
  cat(paste0(Sys.time(), " starting CV.\n", 
             (nrow(tgrid_RF) + sum(tgrid_RF[, "nrounds"])) * nfolds, " forests to fit!\n",
             length(clust)," cores will be occupied by this process!"))
  
   # loop over folds each is used as test set once
   parallel::parLapply(clust, names(folds), \(f_ind){

    # test - bool 
    test_bool <- names(folds) %in% f_ind

    # train and test
    train_init <- data[do.call(c, folds[!test_bool]), ]
    test_init <- data[folds[test_bool] |> unlist(), ]
    
    # train model using all features and CV input
     apply(tgrid_RF, 1, \(cv_inp){

      # fit
      ranger::ranger(Winning_Bid ~., mtry = floor(as.numeric(cv_inp[1]) * ncol(train_init)),
                     splitrule = cv_inp[2],
                     min.node.size = as.numeric(cv_inp[3]),
                     max.depth = as.numeric(cv_inp[4]), 
                     num.trees = as.numeric(cv_inp[5]),
                     data = train_init,
                     importance = importance) -> fit_rf
       
       # importance
       importance <- fit_rf[["variable.importance"]]
       
       # sort and choose
       names_sub <- sort(importance, 
                         decreasing = TRUE)[1:floor(length(importance) * var_share)] |> names()
          
       # data to be overwritten (we will remove features from this object)
       # first we take var_share * columns of the most important variables
       dat_it <- train_init[, c("Winning_Bid", names_sub)] 
       
       # recursive feature elimination
       for(i in 1:cv_inp[6]){
         
         # fit rf
         ranger::ranger(Winning_Bid ~., mtry = floor(as.numeric(cv_inp[1]) * ncol(dat_it)),
                        splitrule = cv_inp[2],
                        min.node.size = as.numeric(cv_inp[3]),
                        max.depth = as.numeric(cv_inp[4]), 
                        num.trees = as.numeric(cv_inp[5]),
                        data = dat_it,
                        importance = importance) -> fit_rf_it
         
         # importance 
         importance_it <- fit_rf_it[["variable.importance"]]
      
         # sort and choose
         names_sub_it <- sort(importance_it, 
                              decreasing = TRUE)[1:floor(length(importance_it) * var_share)] |> names()
         
         # overwrite data 
         dat_it <- dat_it[, c("Winning_Bid", names_sub_it)]

         # in last recursive call write performance into storage
         if(i == as.numeric(cv_inp[6])){

           # final rf
           ranger::ranger(Winning_Bid ~., mtry = floor(as.numeric(cv_inp[1]) * ncol(dat_it)),
                          splitrule = cv_inp[2],
                          min.node.size = as.numeric(cv_inp[3]),
                          max.depth = as.numeric(cv_inp[4]), 
                          num.trees = as.numeric(cv_inp[5]),
                          data = dat_it,
                          importance = importance) -> fit_rf_it
            
           # generate test set with all chosen variables
           dat_test <- test_init[, c("Winning_Bid", names_sub_it)]
           
           # predict on test set
           pred <- predict(fit_rf_it, dat_test)
 
           # eval
           eval_res <- eval_fun(actual = dat_test[, "Winning_Bid"],
                                predicted = pred[["predictions"]])
           
           # save and return performance 
           return(c(cv_inp[1], # feat_share
                    cv_inp[2], # splitrule
                    cv_inp[3], # min.node.size
                    cv_inp[4], # max.depth
                    cv_inp[5], # ntrees
                    cv_inp[6], # nrounds 
                    "var_share" = var_share,
                    "performance" = eval_res))
        }
 
       }
    
      }) |> as.data.frame() |> setNames(paste0("RF_", 1:nrow(tgrid_RF)))
    
  }) |> setNames(1:nfolds) -> tmp
  
  # release cores 
  on.exit(parallel::stopCluster(clust), add = TRUE)
  
  # return
  return(tmp)
  
}
```

# Execute

```{r}
rfe_rf_CV_par(data = dat_aucs_mod[["Train"]], nfolds = 5, splitrule_CV = "variance",
               min_node_size_CV = seq(1, 3, 2), max_depth_CV = seq(5, 35, 15),
               num_trees_CV = c(1500, 2000), var_share = 0.75, 
               nrounds = seq(12, 16, 2),
               feat_share = c(0.8, 0.85),
               eval_fun = Metrics::rmse,
               ncore = NULL,
               seed = 33)
```

# Evaluate First Run Colab Training Results 

```{r}
# training results run 1
train_res <- readRDS("./../../Data/Models/RF/RFE/NestedCV_rfe_rf.RDS")

# average performance over folds
rmse <- do.call(rbind, lapply(train_res, \(mat) mat[nrow(mat), ] |> as.numeric())) |> colMeans()

# snip
per_rank <- train_res[[1]]

# asssign
per_rank[nrow(per_rank), ] <- rmse

# obtain performance ranking
per_rank <- per_rank[, order(per_rank[nrow(per_rank), ])]

# best model
best_fit <- per_rank[, 1, drop = FALSE]
```

# Evaluate Second Run Colab Training Results

```{r}
# training results run 1
train_res <- readRDS("./../../Data/Models/RF/RFE/NestedCV_rfe_rf_r2.RDS")

# average performance over folds
rmse <- do.call(rbind, lapply(train_res, \(mat) mat[nrow(mat), ] |> as.numeric())) |> colMeans()

# snip
per_rank <- train_res[[1]]

# asssign
per_rank[nrow(per_rank), ] <- rmse

# obtain performance ranking
per_rank <- per_rank[, order(per_rank[nrow(per_rank), ])]

# best model
best_fit <- per_rank[, 1, drop = FALSE]
```

# Predict Holdout Set

```{r}
fit_rfe_rf <- \(formula, data, splitrule = "variance",
                min_node_size = 5, max_depth = 20,
                num_trees = 1000, var_share = 0.6, 
                nrounds = 10,
                feat_share = 0.9,
                importance = "permutation"){
  
  # hot encode data
  data <- model.matrix(~. + 0, data = data) |> as.data.frame()
  
  # ensure viable colnames
  names(data) <- make.names(names(data))
  
  # initial fit to evaluate
  ranger::ranger(formula = formula, data = data, num.trees = num_trees,
                 mtry = floor(feat_share * ncol(data)),
                 splitrule = splitrule,
                 min.node.size = min_node_size,
                 max.depth = max_depth, importance = importance) -> fit
  
  
  # importance 
  importance_it <- fit[["variable.importance"]]

  # sort and choose
  names_sub_it <- sort(importance_it, 
                      decreasing = TRUE)[1:floor(length(importance_it) * var_share)] |> names()
 
  # overwrite data 
  dat_it <- data[, c("Winning_Bid", names_sub_it)]

  
  # recursive elimination
  for(i in 1:nrounds){
   
   # fit rf
   ranger::ranger(formula = formula, data = dat_it, num.trees = num_trees,
                  mtry = floor(feat_share * ncol(dat_it)),
                  splitrule = splitrule,
                  min.node.size = min_node_size,
                  max.depth = max_depth, importance = importance) -> fit_rf_it
   
   # importance 
   importance_it <- fit_rf_it[["variable.importance"]]

   # sort and choose
   names_sub_it <- sort(importance_it, 
                        decreasing = TRUE)[1:floor(length(importance_it) * var_share)] |> names()
   
   # overwrite data 
   dat_it <- dat_it[, c("Winning_Bid", names_sub_it)]

   # in last recursive call write performance into storage
   if(i == nrounds){

     # final rf
     ranger::ranger(formula = formula, data = dat_it, num.trees = num_trees,
                    mtry = floor(feat_share * ncol(dat_it)),
                    splitrule = splitrule,
                    min.node.size = min_node_size,
                    max.depth = max_depth, importance = importance) -> fit_rf_it
     
     # return  model  
     return(list("Model" = fit_rf_it,
                 "Variables" = names_sub_it))
  }

 }
  
}
```

```{r}
# to numeric
best_fit <- sapply(best_fit, as.numeric)

# fit
fit <- fit_rfe_rf(Winning_Bid ~ ., data = dat_aucs_mod[["Train"]],
                  min_node_size = best_fit[3],
                  max_depth = best_fit[4],
                  num_trees = best_fit[5],
                  nrounds = best_fit[6],
                  feat_share = best_fit[1],
                  var_share = best_fit[7])

# predict on test set
pred <- predict(fit$Model, dat_aucs_mod[["Test"]][, fit$Variables])

# write predictions into folder
saveRDS(pred[["predictions"]], "./../../Data/Misc Data/Figure_Data/rfe_rf_pred.RDS")
```

