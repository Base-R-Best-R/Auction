---
title: |
       | Pre-Process Forest
author: "Fabian Blasch"
date: "`r format(Sys.Date(), format = '%m/%d/%Y')`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{float}
   - \usepackage{titling}
   - \usepackage{xcolor}
output: 
   pdf_document:
      number_sections: TRUE
---

# Load Data

```{r, results = "hide", message = FALSE, warning = FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE,
                      fig.pos = "center",
                      fig.width = 8,
                      fig.height = 4,
                      fig.pos = "H")

# source AUX
source("./../Misc/Auxilliary.R")
source("./../Misc/model_eval.R")

# packages
get.package(c("lubridate", "ranger", "tidyverse", "patchwork", "logisticPCA", "Metrics"))

# load data
dat_aucs_eng <- readRDS("./../../Data/Bid Tab RDS/Aucs_df_feateng_split.RDS")
```

# Custom Preprocessed CV PCA

Unfortunately, the cross-validation procedure that utilizes logistic PCA on groups of the CV-folds is not implemented, neither in R's caret or mlr3 nor in ptyhon's scikit-learn. Accordingly, we will implement a custom CV function for grouped logistics PCA followed by a random forest fit.

```{r}
# separate train
aucs_train <- dat_aucs_eng[["Train"]]
```

```{r}
# write CV function
log_PCA_rf_CV <- \(data, nfolds = 5, m_nest_CV = seq(1, 13, 2), 
                   k_desc_CV = seq(2, 20, 2), k_ven_CV = seq(2, 20, 2), 
                   k_vendInt_CV = seq(2, 20, 2), splitrule_CV = "variance",
                   min_node_size_CV = 1:4, max_depth_CV = seq(5, 70, 5), mtry_incr = 5,
                   num_trees_CV = 1000, mtry_incr_start = 5, eval_fun = Metrics::rmse){
      
  # rm unwanted cols
  within(data,{
    Contract_ID <- NULL
    MLOT <- NULL
    EW_Diff <- NULL
    Winning_Bid <- Winning_Bid / 1e3
    Eng_Est <- Eng_Est / 1e3
  }) -> data

  
  # bools
  vend_cols_log <- names(data) |> stringr::str_detect("Vend_") 
  interact_cols <- names(data) |> stringr::str_detect("_x_")
  descr_words <- which(!vend_cols_log)[-c(1:7)]
  Vend_not_int <- vend_cols_log & (!interact_cols)
  
  # generate CV folds
  folds <- sample(nrow(data), nrow(data), replace = FALSE) |> 
            split(as.factor(1:nfolds)) |> setNames(1:nfolds) |> suppressWarnings()

  # tuning grid init
  tgrid_PC <- expand.grid("nPCA_Desc" = k_desc_CV,
                          "nPCA_Vend" = k_ven_CV,
                          "nPCA_VendInt" = k_vendInt_CV)
  
  # fold count
  fold_count <- 1
  
  # loop over folds each is used as test set once
  lapply(names(folds), \(f_ind){
    
      # test - bool 
      test_bool <- names(folds) %in% f_ind

      # train and test
      train <- data[do.call(c, folds[!test_bool]), ]
      test <- data[folds[test_bool] |> unlist(), ]
      
      # separate into desc / Vend / Vend_int
      grps <- list("Train" = list("Description" = train[, descr_words], 
                                  "Vendor" = train[, Vend_not_int],
                                  "Vendor_Interaction" = train[, interact_cols]),
                   "Test" = list("Description" = test[, descr_words], 
                                  "Vendor" = test[, Vend_not_int],
                                  "Vendor_Interaction" = test[, interact_cols]))
      
      # Print Fold Start
      cat(paste0(Sys.time(), ", starting CV for fold: ", fold_count, "/", nfolds, "\n"))
      fold_count <<- fold_count + 1
      
      # nested CV
      apply(tgrid_PC, 1, \(x){

        # Over all 3 binary subsets
        Map(\(dat_train, dat_test, kk){

          # CV for m of logistic PCA
          cv_PCA <- cv.lpca(dat_train, ms = m_nest_CV, ks = kk)
        
          # fit
          fit_PCA <- logisticPCA(dat_train, k = kk, 
                                 m = colnames(cv_PCA)[which.min(cv_PCA)] |> as.numeric())
            
          # predict 
          pred_PCA <- predict(fit_PCA, dat_test, type = "PCs") # name this (variable importance)
          
          # return
          return(list("Fit_PCA" = fit_PCA,
                      "Pred_PCA" = pred_PCA))
          
        }, grps[["Train"]], grps[["Test"]], x) -> fitted_PCs

        # assemble PCA dataset
        PC_dfs <- Map(\(tt, PCAtt, bool){
    
          # bool for supset
          if(bool){
            
            # subset from fit
            PCs <- lapply(fitted_PCs, "[[", PCAtt) |> lapply(\(z) as.data.frame(z[["PCs"]])) 
              
          } else {
            
            PCs <- lapply(fitted_PCs, \(t) as.data.frame(t[[PCAtt]]))
            
          }
    
          # assemble new test and train set
          cbind(tt[1:7], do.call(cbind, PCs))
          
        }, list(train, test), 
        c("Fit_PCA", "Pred_PCA"), c(TRUE, FALSE)) |> setNames(c("Train", "Test"))

        # mtry value depending on number of PCs and thus added now
        mtry_CV <- seq(mtry_incr_start, ncol(PC_dfs[["Train"]]), mtry_incr)
        
        # RF tuning grid 
        tgrid_RF <- expand.grid("mtry" = mtry_CV,
                                "splitrule" = splitrule_CV,
                                "min_node_size" = min_node_size_CV,
                                "max_depth" = max_depth_CV, 
                                "num_trees" = num_trees_CV)
        
        ## Random forest ##
        apply(tgrid_RF, 1, \(cv_inp){
          
          # print
          cat("Growing trees ...\n")
    
          # fit
          ranger::ranger(Winning_Bid ~., mtry = as.numeric(cv_inp[1]),
                         splitrule = cv_inp[2],
                         min.node.size = as.numeric(cv_inp[3]),
                         max.depth = as.numeric(cv_inp[4]), 
                         num.trees = as.numeric(cv_inp[5]),
                         data = PC_dfs[["Train"]]) -> fit_rf
          
          # predict 
          pred <- predict(fit_rf, PC_dfs[["Test"]])
          
          # calc RMSE
          eval_res <- eval_fun(actual = PC_dfs[["Test"]][, "Winning_Bid"], predicted = pred[["predictions"]])
          
          # return result and inputs
          return(c(cv_inp[1], # mtry
                   cv_inp[2], # splitrule
                   cv_inp[3], # min.node.size
                   cv_inp[4], # max.depth
                   cv_inp[5], # ntrees
                   x[1], # nPCA Desc
                   x[2], # nPCA Vend
                   x[3], # nPCA VendInt
                   "performance" = eval_res)) 
          
      }) |> as.data.frame() |> setNames(paste0("RF_", 1:nrow(tgrid_RF)))
    }) |> setNames(paste0("PCA_", 1:nrow(tgrid_PC)))
  }) |> setNames(1:nfolds) 
}

# ex
log_PCA_rf_CV(dat_aucs_eng[["Train"]], nfolds = 5, m_nest_CV = c(3, 8, 12), 
              k_desc_CV = c(10, 20, 30), k_ven_CV = c(10, 20, 30), 
              k_vendInt_CV = c(10, 20, 30), splitrule_CV = "variance",
              min_node_size_CV = c(1, 3, 5), max_depth_CV = c(30, 50, 70, 110), 
              mtry_incr = 5) -> res
```

# Parallell Version of logPCA PP

```{r}
# write CV function
log_PCA_rf_CV_par <- \(data, nfolds = 5, m_nest_CV = seq(1, 13, 2), 
                   k_desc_CV = seq(2, 20, 2), k_ven_CV = seq(2, 20, 2), 
                   k_vendInt_CV = seq(2, 20, 2), splitrule_CV = "variance",
                   min_node_size_CV = 1:4, max_depth_CV = seq(5, 70, 5), mtry_incr = 5,
                   num_trees_CV = 1500, mtry_incr_start = 5, eval_fun = Metrics::rmse,
                   ncore = NULL){
      
  # rm unwanted cols
  within(data,{
    Contract_ID <- NULL
    MLOT <- NULL
    EW_Diff <- NULL
    Winning_Bid <- Winning_Bid / 1e3
    Eng_Est <- Eng_Est / 1e3
  }) -> data

  
  # bools
  vend_cols_log <- names(data) |> stringr::str_detect("Vend_") 
  interact_cols <- names(data) |> stringr::str_detect("_x_")
  descr_words <- which(!vend_cols_log)[-c(1:7)]
  Vend_not_int <- vend_cols_log & (!interact_cols)
  
  # generate CV folds
  folds <- sample(nrow(data), nrow(data), replace = FALSE) |> 
            split(as.factor(1:nfolds)) |> setNames(1:nfolds) |> suppressWarnings()

  # tuning grid init
  tgrid_PC <- expand.grid("nPCA_Desc" = k_desc_CV,
                          "nPCA_Vend" = k_ven_CV,
                          "nPCA_VendInt" = k_vendInt_CV)
  
  # set up parallel compute cluster
  if(is.null(ncore)){
    
    # set amount of cores to the max available and leave one out
    ncore <- parallel::detectCores() - 1
    
    # we parallelize over folds - the maximum number of occupied cores should thus be
    ncore <- min(ncore, nfolds)
    
  } else {
    
    # find min of ncore and folds
    ncore <- min(ncore, nfolds)
    
  }

  # set up cluster
  clust <- parallel::makeCluster(ncore, outfile = "")

  # print cores that will be occupied
  cat(paste0(length(clust), " cores will be occupied by this process!"))
  
  # loop over folds each is used as test set once
  parallel::parLapply(clust, names(folds), \(f_ind){
    
      # test - bool 
      test_bool <- names(folds) %in% f_ind

      # train and test
      train <- data[do.call(c, folds[!test_bool]), ]
      test <- data[folds[test_bool] |> unlist(), ]
      
      # separate into desc / Vend / Vend_int
      grps <- list("Train" = list("Description" = train[, descr_words], 
                                  "Vendor" = train[, Vend_not_int],
                                  "Vendor_Interaction" = train[, interact_cols]),
                   "Test" = list("Description" = test[, descr_words], 
                                  "Vendor" = test[, Vend_not_int],
                                  "Vendor_Interaction" = test[, interact_cols]))
      
      # nested CV
      apply(tgrid_PC, 1, \(x){

        # Over all 3 binary subsets
        Map(\(dat_train, dat_test, kk){

          # CV for m of logistic PCA
          cv_PCA <- logisticPCA::cv.lpca(dat_train, ms = m_nest_CV, ks = kk)
        
          # fit
          fit_PCA <- logisticPCA::logisticPCA(dat_train, k = kk, 
                                 m = colnames(cv_PCA)[which.min(cv_PCA)] |> as.numeric())
            
          # predict 
          pred_PCA <- predict(fit_PCA, dat_test, type = "PCs") # name this (variable importance)
          
          # return
          return(list("Fit_PCA" = fit_PCA,
                      "Pred_PCA" = pred_PCA))
          
        }, grps[["Train"]], grps[["Test"]], x) -> fitted_PCs

        # assemble PCA dataset
        PC_dfs <- Map(\(tt, PCAtt, bool){
    
          # bool for supset
          if(bool){
            
            # subset from fit
            PCs <- lapply(fitted_PCs, "[[", PCAtt) |> lapply(\(z) as.data.frame(z[["PCs"]])) 
              
          } else {
            
            PCs <- lapply(fitted_PCs, \(t) as.data.frame(t[[PCAtt]]))
            
          }
    
          # assemble new test and train set
          cbind(tt[1:7], do.call(cbind, PCs))
          
        }, list(train, test), 
        c("Fit_PCA", "Pred_PCA"), c(TRUE, FALSE)) |> setNames(c("Train", "Test"))

        # mtry value depending on number of PCs and thus added now
        mtry_CV <- seq(mtry_incr_start, ncol(PC_dfs[["Train"]]), mtry_incr)
        
        # RF tuning grid 
        tgrid_RF <- expand.grid("mtry" = mtry_CV,
                                "splitrule" = splitrule_CV,
                                "min_node_size" = min_node_size_CV,
                                "max_depth" = max_depth_CV, 
                                "num_trees" = num_trees_CV)
        
        ## Random forest ##
        apply(tgrid_RF, 1, \(cv_inp){
          
          # fit
          ranger::ranger(Winning_Bid ~., mtry = as.numeric(cv_inp[1]),
                         splitrule = cv_inp[2],
                         min.node.size = as.numeric(cv_inp[3]),
                         max.depth = as.numeric(cv_inp[4]), 
                         num.trees = as.numeric(cv_inp[5]),
                         data = PC_dfs[["Train"]]) -> fit_rf
          
          # predict 
          pred <- predict(fit_rf, PC_dfs[["Test"]])
          
          # calc RMSE
          eval_res <- eval_fun(actual = PC_dfs[["Test"]][, "Winning_Bid"], predicted = pred[["predictions"]])
          
          # return result and inputs
          return(c(cv_inp[1], # mtry
                   cv_inp[2], # splitrule
                   cv_inp[3], # min.node.size
                   cv_inp[4], # max.depth
                   cv_inp[5], # ntrees
                   x[1], # nPCA Desc
                   x[2], # nPCA Vend
                   x[3], # nPCA VendInt
                   "performance" = eval_res)) 
          
      }) |> as.data.frame() |> setNames(paste0("RF_", 1:nrow(tgrid_RF)))
    }) |> setNames(paste0("PCA_", 1:nrow(tgrid_PC)))
  }) |> setNames(1:nfolds) -> tmp
  
  # release cores 
  on.exit(parallel::stopCluster(clust), add = TRUE)
  
  # return
  return(tmp)
  
}

# ex
log_PCA_rf_CV_par(dat_aucs_eng[["Train"]], nfolds = 5, m_nest_CV = c(3, 8, 12), 
              k_desc_CV = c(10, 20, 30), k_ven_CV = c(10, 20, 30), 
              k_vendInt_CV = c(10, 20, 30), splitrule_CV = "variance",
              min_node_size_CV = c(1, 3, 5), max_depth_CV = c(30, 50, 70, 110), 
              mtry_incr = 5) -> res

```

# Variable Selection 

```{r}
# write CV function
Vsel_rf_CV_par <- \(data, nfolds = 5, splitrule_CV = "variance",
                   min_node_size_CV = 1:4, max_depth_CV = seq(5, 70, 5), mtry_incr = 5,
                   num_trees_CV = 1500, var_share = seq(), mtry_incr_start = 5, 
                   eval_fun = Metrics::rmse,
                   ncore = NULL){
  
  # 
  
}
```


# Evaluate Tuning Results From Colab

```{r}
# cross validation results list
CV_lst <- readRDS("./../../Data/Models/RF/PCA/NestedCV_logPCA_rf.RDS")

# function for list picking
lst_pick <- \(lst, where) lapply(lst, "[[", where)

# calculate average performance of each model across all 5 folds
do.call(cbind, lapply(1:length(CV_lst[[1]]), \(ind){

  # subset 
  mods <- lst_pick(CV_lst, ind)

  # extract performance
  pers <- lapply(mods, \(per){

    # performance
    list(#per[nrow(per) - 1, ] |> as.numeric(),
         per[nrow(per), ] |> as.numeric())
    
    
  }) 

  # lst pick
  rmses <- do.call(rbind, lst_pick(pers, 1)) |> colMeans()
  #maes <- do.call(rbind, lst_pick(pers, 2)) |> colMeans()

  # return back data frame with inputs
  mods[[1]][c(9) ,] <- rmses#rbind(rmses, maes)
  
  # return
  mods[[1]]
  
})) -> per_df

# obtain performance ranking
per_rank <- per_df[, order(per_df[9, ])]

# best model
best_fit <- per_rank[, 1, drop = FALSE]
```

# Performance on Test Data

```{r}
# rm
# lapply(dat_aucs_eng, \(dat){
#   
#   # rm vars
#   within(dat, {
#     Contract_ID <- NULL
#     MLOT <- NULL
#     EW_Diff <- NULL
#     Winning_Bid <- Winning_Bid / 1e3
#     Eng_Est <- Eng_Est / 1e3
#   }) 
#   
# }) -> dat_aucs_mod

# train and test
# train <- dat_aucs_mod[["Train"]]
# test <- dat_aucs_mod[["Test"]]


# bools
# vend_cols_log <- names(train) |> stringr::str_detect("Vend_") 
# interact_cols <- names(train) |> stringr::str_detect("_x_")
# descr_words <- which(!vend_cols_log)[-c(1:7)]
# Vend_not_int <- vend_cols_log & (!interact_cols)

# separate into desc / Vend / Vend_int
# grps <- list("Train" = list("Description" = train[, descr_words], 
#                             "Vendor" = train[, Vend_not_int],
#                             "Vendor_Interaction" = train[, interact_cols]),
#              "Test" = list("Description" = test[, descr_words], 
#                             "Vendor" = test[, Vend_not_int],
#                             "Vendor_Interaction" = test[, interact_cols]))

# fitting log PCA
# Map(\(train, test, nPCA){
# 
#   # CV for m of logistic PCA
#   cv_PCA <- cv.lpca(train, ms = c(2, 4, 8, 12), ks = nPCA)
# 
#   # fit
#   fit_PCA <- logisticPCA(train, k = nPCA, 
#                          m = colnames(cv_PCA)[which.min(cv_PCA)] |> as.numeric())
#     
#   # predict 
#   pred_PCA <- predict(fit_PCA, test, type = "PCs") 
#   
#   # return
#   return(list("Fit_PCA" = fit_PCA,
#               "Pred_PCA" = pred_PCA))
#   
# }, grps[[1]], grps[[2]], c(20, 10, 20)) -> fitted_PCs

# assemble PCA dataset
# PC_dfs <- Map(\(tt, PCAtt, bool){
# 
#   # bool for supset
#   if(bool){
#     
#     # subset from fit
#     PCs <- lapply(fitted_PCs, "[[", PCAtt) |> lapply(\(z) as.data.frame(z[["PCs"]])) 
#       
#   } else {
#     
#     PCs <- lapply(fitted_PCs, \(t) as.data.frame(t[[PCAtt]]))
#     
#   }
# 
#   # assemble new test and train set
#   cbind(tt[1:7], do.call(cbind, PCs))
#   
# }, list(train, test), 
# c("Fit_PCA", "Pred_PCA"), c(TRUE, FALSE)) |> setNames(c("Train", "Test"))

# write file 
# saveRDS(PC_dfs, "./../../Data/Misc Data/rf_logPCA_dfs_split.RDS")
```

```{r}
# read train and test set
PC_dfs <- readRDS("./../../Data/Misc Data/rf_logPCA_dfs_split.RDS")

# fit model on training data
ranger::ranger(Winning_Bid ~., mtry = as.numeric(best_fit[1, 1]),
                         splitrule = best_fit[2, 1],
                         min.node.size = as.numeric(best_fit[3, 1]),
                         max.depth = as.numeric(best_fit[4, 1]), 
                         num.trees = as.numeric(best_fit[5, 1]),
                         data = PC_dfs[["Train"]]) -> fit_train

# predict training set
predicted_vals <- predict(fit_train, PC_dfs[["Test"]])[["predictions"]]

# write rds
# saveRDS(as.data.frame(cbind(predicted_vals, "RF")), "./../../Data/Misc Data/logPCA_RF_pred.RDS")

# eval
par(mfrow = c(1, 3))
boxplot(PC_dfs[["Test"]][["Winning_Bid"]], ylim = c(0, 20e3), main = "Actual",
        pch = 19, outcol = "darkblue", col = "cornflowerblue")
boxplot(predicted_vals , ylim = c(0, 20e3), main = "Predicted", pch = 19, outcol = "darkblue",
        col = "cornflowerblue")
boxplot(PC_dfs[["Test"]][["Eng_Est"]], ylim = c(0, 20e3), main = "Eng_Est", pch = 19, outcol = "darkblue",
        col = "cornflowerblue")

# rmse
sapply(list(predicted_vals, PC_dfs[["Test"]][["Eng_Est"]]), \(x){
  
  sapply(list(Metrics::rmse, Metrics::mae), \(fun){
    
      fun(PC_dfs[["Test"]][["Winning_Bid"]], x)
    
  }) |> setNames(c("RMSE", "MAE"))

}) |> knitr::kable(col.names = c("RF", "Eng. Estimate"))
```

















